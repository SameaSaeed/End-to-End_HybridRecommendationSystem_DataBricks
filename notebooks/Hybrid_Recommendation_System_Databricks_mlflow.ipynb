{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2031cdea-7a3b-4a32-9021-cbbc823115a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.session.SparkSession at 0xffd8d3f1aab0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b336f0f-4365-4869-8745-45f1494fc2e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_movie = spark.read.csv(\"/Volumes/workspace/default/data/movies.csv\", header=True, inferSchema=True)\n",
    "df_rating = spark.read.csv(\"/Volumes/workspace/default/data/ratings.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02e49926-a976-433f-aa0a-383711f6620e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = (\n",
    "    df_rating.join(df_movie, on=\"movieId\", how=\"inner\")\n",
    "             .orderBy(col(\"userId\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68d8acee-7cae-484b-83b5-53f0e37e4921",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "user_movie_df = (\n",
    "    df\n",
    "        .groupBy(\"userId\")\n",
    "        .pivot(\"title\")\n",
    "        .agg(F.first(\"rating\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cc66b5a-a0d1-48f0-bbd6-f4870d42de87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.spark  # for PySpark models\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Split data into training and test sets\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "mlflow.set_experiment(\"/Users/samiasaeed0006@gmail.com/hybrid-recommender\")\n",
    "\n",
    "# Build recommendation model using ALS\n",
    "als = ALS(\n",
    "    maxIter=5,\n",
    "    regParam=0.01,\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"movieId\",\n",
    "    ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\"\n",
    ")\n",
    "model = als.fit(train)\n",
    "\n",
    "# Save the model to UC volume path\n",
    "# model.write().overwrite().save(\"/Volumes/workspace/default/models\")\n",
    "\n",
    "# Generate predictions\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\",\n",
    "    labelCol=\"rating\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) = \" + str(rmse))\n",
    "\n",
    "# Take a small sample and convert to pandas\n",
    "input_example = test.limit(3).toPandas()\n",
    "output_example = model.transform(test.limit(3)).toPandas()\n",
    "\n",
    "from mlflow.models.signature import infer_signature\n",
    "signature = infer_signature(input_example, output_example)\n",
    "\n",
    "with mlflow.start_run(run_name=\"HybridRecommender\") as run:\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"rank\", 10)\n",
    "    mlflow.log_param(\"maxIter\", 5)\n",
    "    mlflow.log_param(\"regParam\", 0.01)\n",
    "    \n",
    "    # Log metric\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    \n",
    "    mlflow.spark.log_model(\n",
    "    spark_model=model,\n",
    "    artifact_path=\"als_model\",\n",
    "    registered_model_name=\"HybridRecommender\",\n",
    "    dfs_tmpdir=\"/Volumes/workspace/default/my_uc_volume/tmp_mlflow\",\n",
    "    signature=signature,\n",
    "    input_example=input_example\n",
    "    )\n",
    "    \n",
    "    print(f\"Model registered in run: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bce4117-1b90-473d-9d7f-e5303bbeb7f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%fs mkdirs /Volumes/workspace/default/my_uc_volume/tmp_mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d45efe3-e01e-47f6-b067-cdcddd54043b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Item-Item CF - Similarity Computation\n",
    "mean_ratings = df.groupBy(\"movieId\").agg(F.avg(\"rating\").alias(\"mean_rating\"))\n",
    "normalized_ratings_df = df.join(mean_ratings, \"movieId\")\n",
    "normalized_ratings_df = normalized_ratings_df.withColumn(\"norm_rating\", col(\"rating\") - col(\"mean_rating\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f26b9a9-02ba-49e8-8c1f-d3e63b2e92c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, sqrt, sum as sql_sum, when, lit\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, count, sqrt, lit, when\n",
    "\n",
    "def compute_similarity(df):\n",
    "    # Perform join on userId and ensure correct column names are used for movieId\n",
    "    joined_df = df.alias(\"df1\").join(df.alias(\"df2\"), \"userId\")\n",
    "    \n",
    "    # Ensure we are comparing different movieId pairs (df1.movieId < df2.movieId)\n",
    "    joined_df = joined_df.filter(\"df1.movieId < df2.movieId\")\n",
    "    \n",
    "    # Perform aggregation to calculate similarity components\n",
    "    joined_df = joined_df.groupBy(\"df1.movieId\", \"df2.movieId\").agg(\n",
    "        count(col(\"df1.movieId\")).alias(\"numPairs\"),\n",
    "        F.sum(col(\"df1.norm_rating\") * col(\"df2.norm_rating\")).alias(\"sum_xy\"),\n",
    "        F.sum(col(\"df1.norm_rating\") * col(\"df1.norm_rating\")).alias(\"sum_xx\"),\n",
    "        F.sum(col(\"df2.norm_rating\") * col(\"df2.norm_rating\")).alias(\"sum_yy\")\n",
    "    )\n",
    "\n",
    "    # Compute similarity (numerator and denominator)\n",
    "    result_df = joined_df.withColumn(\"numerator\", col(\"sum_xy\"))\n",
    "    result_df = result_df.withColumn(\"denominator\", sqrt(col(\"sum_xx\")) * sqrt(col(\"sum_yy\")))\n",
    "    result_df = result_df.withColumn(\"similarity\",\n",
    "                                     when(col(\"denominator\") != 0, col(\"numerator\") / col(\"denominator\"))\n",
    "                                     .otherwise(lit(0)))\n",
    "\n",
    "    # Return the resulting DataFrame with movieId1, movieId2, and similarity\n",
    "    return result_df.select(col(\"df1.movieId\").alias(\"movieId1\"), \n",
    "                            col(\"df2.movieId\").alias(\"movieId2\"), \n",
    "                            \"similarity\")\n",
    "\n",
    "# Compute similarity and save to Parquet\n",
    "movie_similarity_df = compute_similarity(normalized_ratings_df)\n",
    "movie_similarity_df.write.mode(\"overwrite\").parquet(\"/Volumes/workspace/default/models/item_item_similarities\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72e7d7dd-efbf-454d-b513-c9360a784255",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, rank\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "def calculate_item_cf_predictions(user_movie_pairs, item_similarity_df, user_ratings_df, N=10):\n",
    "    # Alias the DataFrames for clarity\n",
    "    similarities = item_similarity_df.alias(\"sims\")\n",
    "    ratings = user_ratings_df.alias(\"ratings\")\n",
    "\n",
    "    # Join the user-item pairs with item similarities and user ratings\n",
    "    user_item_sims = user_movie_pairs.alias(\"pairs\").join(\n",
    "        similarities, col(\"pairs.movieId1\") == col(\"sims.movieId1\")  # Corrected column name\n",
    "    ).join(\n",
    "        ratings, (col(\"ratings.userId\") == col(\"pairs.userId\")) & (col(\"ratings.movieId\") == col(\"sims.movieId2\"))\n",
    "    )\n",
    "\n",
    "    # Select necessary columns\n",
    "    user_item_sims = user_item_sims.select(\n",
    "        col(\"pairs.userId\"),\n",
    "        col(\"pairs.movieId1\").alias(\"target_movieId\"),  # Corrected column name\n",
    "        col(\"sims.movieId2\").alias(\"similar_movieId\"),\n",
    "        col(\"sims.similarity\"),\n",
    "        col(\"ratings.rating\").alias(\"similar_movie_rating\")\n",
    "    )\n",
    "\n",
    "    # Window specification for ranking top N similar movies for each user-target movie pair\n",
    "    windowSpec = Window.partitionBy(\"userId\", \"target_movieId\").orderBy(col(\"similarity\").desc())\n",
    "    top_n_similar = user_item_sims.withColumn(\"rank\", rank().over(windowSpec)).filter(col(\"rank\") <= N)\n",
    "\n",
    "    # Aggregate weighted ratings and similarity sum\n",
    "    weighted_ratings = top_n_similar.groupBy(\"userId\", \"target_movieId\").agg(\n",
    "        F.sum(col(\"similarity\") * col(\"similar_movie_rating\")).alias(\"weighted_sum\"),\n",
    "        F.sum(\"similarity\").alias(\"similarity_sum\")\n",
    "    )\n",
    "\n",
    "    # Calculate final predictions, handling division by zero with try_divide\n",
    "    predictions = weighted_ratings.withColumn(\n",
    "        \"prediction\", \n",
    "        F.when(col(\"similarity_sum\") != 0, col(\"weighted_sum\") / col(\"similarity_sum\"))\n",
    "         .otherwise(F.lit(None))  # Return NULL if similarity_sum is zero\n",
    "    ).select(\"userId\", \"target_movieId\", \"prediction\")\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "468c9e84-46b6-408d-9fc8-6f5d261ec016",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+-----------------+\n",
      "|userId|target_movieId|       prediction|\n",
      "+------+--------------+-----------------+\n",
      "|     1|            13|4.466666666666667|\n",
      "|     1|            30|4.277777777777778|\n",
      "|     1|            47|4.827372638646121|\n",
      "|     1|            48|4.687406724897038|\n",
      "|     1|            55|             NULL|\n",
      "+------+--------------+-----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'test' is your test dataset, 'movie_similarity_df' contains movie similarities, \n",
    "# and 'train' is your training dataset containing ratings\n",
    "\n",
    "# Generate user-movie pairs for prediction\n",
    "user_movie_pairs = test.select(\"userId\").distinct().crossJoin(movie_similarity_df.select(\"movieId1\").distinct())\n",
    "\n",
    "# Generate item CF predictions safely with division by zero handled\n",
    "item_cf_predictions = calculate_item_cf_predictions(user_movie_pairs, movie_similarity_df, train)\n",
    "\n",
    "# Show the first few predictions\n",
    "item_cf_predictions.show(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec6710e0-4733-41bd-a1a6-3d45ca7585e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Received command c on object id p0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Model RMSE: 10.883646393091693\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Combine ALS and Item CF Predictions\n",
    "als_predictions = predictions.withColumnRenamed(\"prediction\", \"als_prediction\")\n",
    "item_cf_predictions = item_cf_predictions.withColumnRenamed(\"prediction\", \"cf_prediction\")\n",
    "item_cf_predictions = item_cf_predictions.withColumnRenamed(\"target_movieId\", \"movieId\")\n",
    "\n",
    "# Join ALS and Item CF predictions on userId and movieId\n",
    "combined_predictions = als_predictions.join(item_cf_predictions, [\"userId\", \"movieId\"], \"inner\")\n",
    "\n",
    "# Calculate the hybrid prediction\n",
    "combined_predictions = combined_predictions.withColumn(\n",
    "    \"hybrid_prediction\",\n",
    "    (F.col(\"als_prediction\") + F.col(\"cf_prediction\")) / 2\n",
    ")\n",
    "\n",
    "# Save the hybrid predictions\n",
    "combined_predictions.write.mode(\"overwrite\").parquet(\"/Volumes/workspace/default/models/hybrid_predictions\")\n",
    "\n",
    "# Optional: Evaluate the hybrid model using RMSE\n",
    "\n",
    "# Rename rating in test to avoid ambiguity\n",
    "test_renamed = test.withColumnRenamed(\"rating\", \"true_rating\")\n",
    "\n",
    "# Join test with combined predictions\n",
    "evaluation_df = test_renamed.join(combined_predictions, [\"userId\", \"movieId\"], \"inner\")\n",
    "\n",
    "# Filter out rows where either 'true_rating' or 'hybrid_prediction' is null\n",
    "evaluation_df = evaluation_df.filter(\n",
    "    evaluation_df[\"true_rating\"].isNotNull() & evaluation_df[\"hybrid_prediction\"].isNotNull()\n",
    ")\n",
    "\n",
    "# Evaluate RMSE\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"true_rating\", predictionCol=\"hybrid_prediction\")\n",
    "hybrid_rmse = evaluator.evaluate(evaluation_df)\n",
    "\n",
    "print(f\"Hybrid Model RMSE: {hybrid_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcd5d957-5dd4-498a-816f-56bd2546f407",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Save metrics to file\n",
    "metrics_data = [\n",
    "    Row(metric=\"ALS RMSE\", value=float(rmse)),\n",
    "    Row(metric=\"Hybrid Model RMSE\", value=float(hybrid_rmse)),\n",
    "]\n",
    "\n",
    "# Create a DataFrame from the metrics data\n",
    "metrics_df = spark.createDataFrame(metrics_data)\n",
    "\n",
    "# Apply coalesce before writing to reduce the number of output files (to 1)\n",
    "metrics_df.coalesce(1).write.format(\"csv\").option(\"header\", \"true\").save(\"/Volumes/workspace/default/models/metrics.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6aa3ff3-7a13-4c62-985a-671a95074601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join ALS, Item CF predictions\n",
    "combined_predictions = als_predictions.join(item_cf_predictions, [\"userId\", \"movieId\"], \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1624a80-0114-417a-a600-4a0a649eadb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hybrid model weights: (0.4, 0.3)\n",
      "Best Hybrid Model RMSE: 6.6516218576976\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Define weight combinations to try\n",
    "weight_values = [\n",
    "    (0.2, 0.3),\n",
    "    (0.3, 0.3),\n",
    "    (0.4, 0.3)\n",
    "]\n",
    "\n",
    "rmse_values = []\n",
    "\n",
    "# Rename rating to avoid ambiguity\n",
    "test_renamed = test.withColumnRenamed(\"rating\", \"true_rating\")\n",
    "\n",
    "for weight_als, weight_cf in weight_values:\n",
    "    # Calculate hybrid prediction\n",
    "    hybrid_df = combined_predictions.withColumn(\n",
    "        \"hybrid_prediction\",\n",
    "        F.col(\"als_prediction\") * weight_als + F.col(\"cf_prediction\") * weight_cf\n",
    "    )\n",
    "\n",
    "    # Join with test data\n",
    "    evaluation_df = hybrid_df.join(test_renamed, [\"userId\", \"movieId\"], \"inner\")\n",
    "\n",
    "    # Remove rows with nulls in label or prediction\n",
    "    evaluation_df = evaluation_df.filter(\n",
    "        (F.col(\"true_rating\").isNotNull()) & (F.col(\"hybrid_prediction\").isNotNull())\n",
    "    )\n",
    "\n",
    "    # Evaluate RMSE\n",
    "    evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"true_rating\", predictionCol=\"hybrid_prediction\")\n",
    "    h2_rmse = evaluator.evaluate(evaluation_df)\n",
    "    rmse_values.append(h2_rmse)\n",
    "\n",
    "# Find best weights\n",
    "best_index = rmse_values.index(min(rmse_values))\n",
    "best_weights = weight_values[best_index]\n",
    "\n",
    "# Save best hybrid predictions\n",
    "best_hybrid_df = combined_predictions.withColumn(\n",
    "    \"hybrid_prediction\",\n",
    "    F.col(\"als_prediction\") * best_weights[0] + F.col(\"cf_prediction\") * best_weights[1]\n",
    ")\n",
    "best_hybrid_df.write.mode(\"overwrite\").parquet(\"/Volumes/workspace/default/models/best_hybrid_model\")\n",
    "\n",
    "print(f\"Best hybrid model weights: {best_weights}\")\n",
    "print(f\"Best Hybrid Model RMSE: {min(rmse_values)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8732a65-f27c-485b-b63d-9e8853ec4edc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.pyfunc\n",
    "from pyspark.sql import functions as F\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "# Define a custom hybrid model class inheriting from mlflow.pyfunc.PythonModel\n",
    "class HybridRecommendationModel(mlflow.pyfunc.PythonModel):\n",
    "    def __init__(self, als_weight=0.5, cf_weight=0.5):\n",
    "        self.als_weight = als_weight\n",
    "        self.cf_weight = cf_weight\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        # model_input is a DataFrame with ALS and CF predictions\n",
    "        als_predictions = model_input['als_prediction']\n",
    "        cf_predictions = model_input['cf_prediction']\n",
    "        \n",
    "        # Calculate hybrid prediction\n",
    "        hybrid_prediction = (als_predictions * self.als_weight) + (cf_predictions * self.cf_weight)\n",
    "        return hybrid_prediction\n",
    "\n",
    "# Log the hybrid model to MLflow\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Set the weights for the hybrid model\n",
    "    hybrid_model = HybridRecommendationModel(als_weight=0.4, cf_weight=0.6)\n",
    "    \n",
    "    # Log the model using mlflow.pyfunc\n",
    "    mlflow.pyfunc.log_model(\"hybrid_model\", python_model=hybrid_model)\n",
    "\n",
    "    # Optionally log parameters if needed\n",
    "    mlflow.log_param(\"als_weight\", 0.4)\n",
    "    mlflow.log_param(\"cf_weight\", 0.6)\n",
    "\n",
    "    # Take a small sample of input and output for signature\n",
    "    input_example = test.limit(3).toPandas()\n",
    "    output_example = model.transform(test.limit(3)).toPandas()\n",
    "\n",
    "    # Infer signature of the model\n",
    "    signature = infer_signature(input_example, output_example)\n",
    "\n",
    "    # Log the model with signature and example input\n",
    "    mlflow.pyfunc.log_model(\n",
    "        \"hybrid_model_with_signature\",\n",
    "        python_model=hybrid_model,\n",
    "        signature=signature,\n",
    "        input_example=input_example\n",
    "    )\n",
    "\n",
    "    print(f\"Model registered in run: {mlflow.active_run().info.run_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f9f4b11-5223-4fc2-a0f7-f4751e74eec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Log predictions (optional)\n",
    "best_hybrid_df = combined_predictions.withColumn(\n",
    "    \"hybrid_prediction\",\n",
    "    F.col(\"als_prediction\") * 0.4 + F.col(\"cf_prediction\") * 0.6\n",
    ")\n",
    "# Assuming you want to log predictions for reference\n",
    "mlflow.log_artifact(\"/Volumes/workspace/default/models/best_hybrid_model\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7208450804016360,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Hybrid_Recommendation_System_Databricks",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
